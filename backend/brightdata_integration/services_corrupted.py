"""
BrightData Integration Services

RESTORED FROM WORKING OLD PROJECT - EXACT IMPLEMENTATION
This module provides services for interacting with the BrightData API
for social media scraping operations using the proven working approach.
"""

import logging
import os
import requests
import json
from typing import Optional, Dict, Any, List
from django.conf import settings
from django.utils import timezone

from .models import BrightDataConfig, BrightDataBatchJob, BrightDataScraperRequest

logger = logging.getLogger(__name__)


class BrightDataAutomatedBatchScraper:
    """Service for managing BrightData batch scraping operations - WORKING IMPLEMENTATION"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.base_url = "https://api.brightdata.com/datasets/v3"
        
        # EXACT WORKING DATASET IDS FROM OLD PROJECT
        self.platform_datasets = {
            'instagram': 'gd_lk5ns7kz21pck8jpis',  # CONFIRMED WORKING Instagram dataset
            'facebook': 'gd_lkaxegm826bjpoo9m5',   # CONFIRMED WORKING Facebook dataset
            'tiktok': 'gd_l7q7dkf244hwps8lu2',     # TikTok dataset ID
            'linkedin': 'gd_l7q7dkf244hwps8lu3',   # LinkedIn dataset ID
        }

    def trigger_scraper(self, platform: str, urls: List[str]) -> Dict[str, Any]:
        """
        DIRECT BRIGHTDATA API TRIGGER - NO DATABASE NEEDED!
        Uses EXACT user-provided format with just API token + dataset IDs
        """
        try:
            self.logger.info(f"ðŸš€ DIRECT API TRIGGER: {platform.upper()} SCRAPER")
            self.logger.info(f"ðŸ“‹ URLs: {urls}")
            
            # Normalize platform name to lowercase for dataset lookup
            platform_lower = platform.lower()
            
            # Get the dataset ID directly from our hardcoded working values
            dataset_id = self.platform_datasets.get(platform_lower)
            if not dataset_id:
                return {'success': False, 'error': f'No dataset ID for platform: {platform_lower}'}
            
            self.logger.info(f"âœ… Using direct API - Dataset ID: {dataset_id}")
            self.logger.info(f"âœ… API Token: 8af6995e-3baa-4b69-9df7-8d7671e621eb")
            
            # DIRECT API CALL - NO DATABASE LOOKUP!
            success, batch_id = self._make_direct_brightdata_request(urls, platform_lower, dataset_id)
            
            if success:
                return {
                    'success': True,
                    'batch_job_id': batch_id or 'batch_created',
                    'platform': platform_lower,
                    'message': f'BrightData {platform_lower} scraper triggered successfully!',
                    'urls_count': len(urls),
                    'dataset_id': dataset_id
                }
            else:
                return {'success': False, 'error': f'Failed to trigger {platform_lower} scraper'}
                
        except Exception as e:
            error_msg = f"Failed to trigger {platform} scraper: {str(e)}"
            self.logger.error(error_msg)
            return {'success': False, 'error': error_msg}

    def _make_direct_brightdata_request(self, urls: List[str], platform: str, dataset_id: str) -> tuple[bool, str]:
        """
        DIRECT BRIGHTDATA API REQUEST - EXACT USER PROVIDED FORMAT!
        No database, no complex logic - just pure API call with token + dataset ID
        """
        try:
            self.logger.info(f"ðŸ”„ DIRECT BrightData API call for {platform}")
            
            # EXACT USER PROVIDED FORMAT
            api_url = "https://api.brightdata.com/datasets/v3/trigger"
            api_token = "8af6995e-3baa-4b69-9df7-8d7671e621eb"
            
            headers = {
                "Authorization": f"Bearer {api_token}",
                "Content-Type": "application/json",
            }
            
            # Base parameters for all platforms
            params = {
                "dataset_id": dataset_id,
                "include_errors": "true",
            }
            
            # Add platform-specific parameters
            if platform == 'instagram':
                params.update({
                    "type": "discover_new",
                    "discover_by": "url",
                })
            
            # Prepare payload with EXACT user format
            payload = []
            for url in urls:
                if platform == 'instagram':
                    # EXACT format from user
                    item = {
                        "url": url,
                        "num_of_posts": 10,
                        "start_date": "01-01-2025",
                        "end_date": "03-01-2025",
                        "post_type": "Post"
                    }
                elif platform == 'facebook':
                    item = {
                        "url": url,
                        "num_of_posts": 10,
                        "start_date": "01-01-2025",
                        "end_date": "03-01-2025"
                    }
                else:
                    item = {"url": url, "num_of_posts": 10}
                
                payload.append(item)
            
            # DEBUG OUTPUT
            print(f"\nðŸ”¥ DIRECT BRIGHTDATA API CALL")
            print(f"Platform: {platform}")
            print(f"Dataset ID: {dataset_id}")
            print(f"API Token: {api_token[:10]}...")
            print(f"Payload: {payload}")
            print(f"URL: {api_url}")
            
            # Make the actual request
            response = requests.post(api_url, headers=headers, params=params, json=payload, timeout=30)
            
            print(f"Response Status: {response.status_code}")
            print(f"Response: {response.text}")
            
            if response.status_code == 200:
                try:
                    response_data = response.json()
                    snapshot_id = response_data.get('snapshot_id', 'direct_batch_created')
                    print(f"âœ… SUCCESS! Snapshot ID: {snapshot_id}")
                    return True, snapshot_id
                except json.JSONDecodeError:
                    print(f"âœ… SUCCESS! (Raw response: {response.text})")
                    return True, "direct_batch_success"
            else:
                print(f"âŒ FAILED! Status: {response.status_code}, Response: {response.text}")
                return False, None
                
        except Exception as e:
            print(f"âŒ EXCEPTION in direct API call: {str(e)}")
            return False, None

    def _make_brightdata_batch_request(self, scraper_requests: List[BrightDataScraperRequest], 
                                     urls: List[str], platform: str) -> bool:
        """
        LEGACY METHOD - Redirects to direct API
        """
        dataset_id = self.platform_datasets.get(platform)
        if not dataset_id:
            return False
        
        success, _ = self._make_direct_brightdata_request(urls, platform, dataset_id)
        return success

    def _get_or_create_config(self, platform: str) -> Optional[BrightDataConfig]:
        """Get or create BrightData configuration for platform - WORKING VERSION"""
        try:
            # Try to get existing config
            config = BrightDataConfig.objects.filter(
                platform=platform,
                is_active=True
            ).first()
            
            if config:
                self.logger.info(f"Found existing config for {platform}")
                return config
            
            # Create default config with WORKING CREDENTIALS FROM OLD PROJECT
            dataset_id = self.platform_datasets.get(platform)
            if not dataset_id:
                self.logger.error(f"No dataset ID available for platform: {platform}")
                return None
            
            config = BrightDataConfig.objects.create(
                name=f"{platform.title()} Posts Scraper",
                platform=platform,
                dataset_id=dataset_id,
                api_token="8af6995e-3baa-4b69-9df7-8d7671e621eb",  # WORKING TOKEN
                is_active=True
            )
            
            self.logger.info(f"Created working config for {platform}: {dataset_id}")
            return config
            
        except Exception as e:
            self.logger.error(f"Error getting/creating config for {platform}: {str(e)}")
            return None

    # ========== LEGACY METHODS FOR BACKWARD COMPATIBILITY ==========
    
    def create_batch_job(self, name: str, project_id: int, source_folder_ids: List[int], 
                        platforms_to_scrape: List[str], content_types_to_scrape: Dict[str, List[str]], 
                        num_of_posts: int = 10, **kwargs) -> Optional[BrightDataBatchJob]:
        """Legacy method for creating batch jobs - redirects to new implementation"""
        urls = kwargs.get('urls', [])
        self.logger.info(f"Legacy create_batch_job called, redirecting to trigger_scraper")
        
        # For each platform, trigger the scraper
        for platform in platforms_to_scrape:
            result = self.trigger_scraper(platform, urls)
            if result['success']:
                self.logger.info(f"Successfully triggered {platform} via legacy method")
            else:
                self.logger.error(f"Failed to trigger {platform} via legacy method: {result.get('error')}")
        
        return None  # Legacy compatibility

    def execute_batch_job(self, batch_job_id: int) -> bool:
        """Legacy method for executing batch jobs - now handled automatically"""
        self.logger.info(f"Legacy execute_batch_job called for ID {batch_job_id}")
        return True
            }

            # Add platform-specific parameters - EXACT FROM OLD PROJECT
            if platform == 'instagram':
                params.update({
                    "type": "discover_new",
                    "discover_by": "url",
                })
            elif platform == 'facebook':
                # Facebook-specific parameters - NO discovery parameters needed
                # The dataset gd_lkaxegm826bjpoo9m5 works with URL payload without discovery params
                pass
            elif platform == 'linkedin':
                # LinkedIn-specific parameters - using profile_url discovery
                params.update({
                    "type": "discover_new",
                    "discover_by": "profile_url",
                })

            # Prepare payload - EXACT USER PROVIDED FORMAT
            payload = []
            for url in urls:
                if platform == 'instagram':
                    # Instagram Posts API format - EXACT USER PROVIDED FORMAT
                    item = {
                        "url": url,
                        "num_of_posts": 10,
                        "start_date": "01-01-2025",
                        "end_date": "03-01-2025",
                        "post_type": "Post"
                    }
                elif platform == 'facebook':
                    # Facebook Posts API format
                    item = {
                        "url": url,
                        "num_of_posts": 50,
                        "start_date": "01-01-2025", 
                        "end_date": "02-28-2025"
                    }
                else:
                    # Generic format
                    item = {
                        "url": url,
                        "num_of_posts": 10
                    }
                
                payload.append(item)

            # DEBUG LOGGING - EXACT FROM OLD PROJECT
            self.logger.info("ðŸ›  BRIGHTDATA API REQUEST DEBUG - RESTORED WORKING VERSION")
            self.logger.info(f"  URL: {url}")
            self.logger.info(f"  Headers: {headers}")
            self.logger.info(f"  Params: {params}")
            self.logger.info(f"  Payload: {payload}")
            self.logger.info(f"  Webhook endpoint: {webhook_base_url}/api/brightdata/webhook/")
            
            print("ðŸ›  BRIGHTDATA API REQUEST DEBUG - RESTORED WORKING VERSION")
            print(f"  Dataset ID: {dataset_id}")
            print(f"  API Token: {api_token[:10]}...")
            print(f"  Platform: {platform}")
            print(f"  Payload items: {len(payload)}")
            print(f"  endpoint: {webhook_base_url}/api/brightdata/webhook/")
            print()

            # Make the actual API request
            response = requests.post(url, headers=headers, params=params, json=payload, timeout=30)

            # DETAILED RESPONSE LOGGING - EXACT FROM OLD PROJECT
            print("\nðŸ”¥ BRIGHTDATA API RESPONSE:")
            print(f"Status Code: {response.status_code}")
            print(f"Response Headers: {dict(response.headers)}")
            print(f"Response Text: {response.text}")
            print("="*80 + "\n")

            if response.status_code == 200:
                try:
                    response_data = response.json()
                except json.JSONDecodeError as json_err:
                    response_data = {
                        "error": "Invalid or empty JSON response",
                        "raw_response": response.text,
                        "json_error": str(json_err)
                    }

                # Update scraper requests with response data
                snapshot_id = response_data.get('snapshot_id') or response_data.get('request_id')
                
                for scraper_request in scraper_requests:
                    if scraper_request:
                        scraper_request.request_id = snapshot_id
                        scraper_request.response_metadata = response_data
                        scraper_request.status = 'processing'
                        scraper_request.started_at = timezone.now()
                        scraper_request.save()

                self.logger.info(f"Successfully triggered batch scrape for {platform} with {len(payload)} sources. Request ID: {snapshot_id}")
                print(f"âœ… SUCCESS! Request ID: {snapshot_id}")
                return True
            else:
                error_msg = f"BrightData API error for {platform} batch: {response.status_code} - {response.text}"
                self.logger.error(error_msg)
                
                # Update scraper requests with error
                for scraper_request in scraper_requests:
                    if scraper_request:
                        scraper_request.status = 'failed'
                        scraper_request.error_message = error_msg
                        scraper_request.save()
                        
                print(f"âŒ FAILED! Status: {response.status_code}, Error: {response.text}")
                return False

        except Exception as e:
            error_msg = f"Exception during BrightData batch request for {platform}: {str(e)}"
            self.logger.error(error_msg)
            
            # Update scraper requests with error
            for scraper_request in scraper_requests:
                if scraper_request:
                    scraper_request.status = 'failed'
                    scraper_request.error_message = error_msg
                    scraper_request.save()
                    
            print(f"âŒ EXCEPTION! Error: {str(e)}")
            print("="*80 + "\n")
            return False

    def _get_or_create_config(self, platform: str) -> Optional[BrightDataConfig]:
        """Get or create BrightData configuration for platform - WORKING VERSION"""
        try:
            # Try to get existing config
            config = BrightDataConfig.objects.filter(
                platform=platform,
                is_active=True
            ).first()
            
            if config:
                self.logger.info(f"Found existing config for {platform}")
                return config
            
            # Create default config with WORKING CREDENTIALS FROM OLD PROJECT
            dataset_id = self.platform_datasets.get(platform)
            if not dataset_id:
                self.logger.error(f"No dataset ID available for platform: {platform}")
                return None
            
            config = BrightDataConfig.objects.create(
                name=f"{platform.title()} Posts Scraper",
                platform=platform,
                dataset_id=dataset_id,
                api_token="8af6995e-3baa-4b69-9df7-8d7671e621eb",  # WORKING TOKEN
                is_active=True
            )
            
            self.logger.info(f"Created working config for {platform}: {dataset_id}")
            return config
            
        except Exception as e:
            self.logger.error(f"Error getting/creating config for {platform}: {str(e)}")
            return None

    # ========== LEGACY METHODS FOR BACKWARD COMPATIBILITY ==========
    
    def create_batch_job(self, name: str, project_id: int, source_folder_ids: List[int], 
                        platforms_to_scrape: List[str], content_types_to_scrape: Dict[str, List[str]], 
                        num_of_posts: int = 10, **kwargs) -> Optional[BrightDataBatchJob]:
        """Legacy method for creating batch jobs - redirects to new implementation"""
        urls = kwargs.get('urls', [])
        self.logger.info(f"Legacy create_batch_job called, redirecting to trigger_scraper")
        
        # For each platform, trigger the scraper
        for platform in platforms_to_scrape:
            result = self.trigger_scraper(platform, urls)
            if result['success']:
                self.logger.info(f"Successfully triggered {platform} via legacy method")
            else:
                self.logger.error(f"Failed to trigger {platform} via legacy method: {result.get('error')}")
        
        return None  # Legacy compatibility

    def execute_batch_job(self, batch_job_id: int) -> bool:
        """Legacy method for executing batch jobs - now handled automatically"""
        self.logger.info(f"Legacy execute_batch_job called for ID {batch_job_id}")
        return True